{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyOCjdfvE0KK",
        "outputId": "9feebf92-a0eb-4268-cedb-e12293c17950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU available: []\n",
            "Result of matrix multiplication:\n",
            "tf.Tensor(\n",
            "[[19. 22.]\n",
            " [43. 50.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# Import TensorFlow and enable eager execution\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "# Check if a TPU is available\n",
        "tpu_available = tf.config.experimental.list_logical_devices('TPU')\n",
        "print(\"TPU available:\", tpu_available)\n",
        "\n",
        "# Define a simple TensorFlow operation\n",
        "x = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "y = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
        "z = tf.matmul(x, y)\n",
        "\n",
        "# Print the result\n",
        "print(\"Result of matrix multiplication:\")\n",
        "print(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r02prEWKNXoQ",
        "outputId": "db49dd63-ba99-4699-ebe8-3e711b2f9e59"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.14.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAxSV0DBNZCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DS6SeRtnB33E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f2932a5-02df-4eca-f1eb-97c342a31a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=18R9RwjJzhmm-6XuU_DxosLRtsrnkNt_Q\n",
            "From (redirected): https://drive.google.com/uc?id=18R9RwjJzhmm-6XuU_DxosLRtsrnkNt_Q&confirm=t&uuid=25f8389f-78b0-43f1-966e-714ba04d6557\n",
            "To: /content/Books_rating.csv\n",
            "100%|██████████| 2.86G/2.86G [00:54<00:00, 52.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the DataFrame:\n",
            "           Id                           Title  Price         User_id  \\\n",
            "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
            "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
            "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
            "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
            "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
            "\n",
            "                          profileName review/helpfulness  review/score  \\\n",
            "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
            "1                       Kevin Killian              10/10           5.0   \n",
            "2                        John Granger              10/11           5.0   \n",
            "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
            "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
            "\n",
            "   review/time                                   review/summary  \\\n",
            "0    940636800           Nice collection of Julie Strain images   \n",
            "1   1095724800                                Really Enjoyed It   \n",
            "2   1078790400  Essential for every personal and Public Library   \n",
            "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
            "4   1107993600                           Good academic overview   \n",
            "\n",
            "                                         review/text  \n",
            "0  This is only for Julie Strain fans. It's a col...  \n",
            "1  I don't care much for Dr. Seuss but after read...  \n",
            "2  If people become the books they read and if \"t...  \n",
            "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
            "4  Philip Nel - Dr. Seuss: American IconThis is b...  \n",
            "Columns in the DataFrame:\n",
            "Index(['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness',\n",
            "       'review/score', 'review/time', 'review/summary', 'review/text'],\n",
            "      dtype='object')\n",
            "Epoch 1/3\n",
            " 8047/75000 [==>...........................] - ETA: 37:26:43 - loss: 1.4582 - root_mean_squared_error: 1.2076"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gdown\n",
        "\n",
        "# Define the Google Drive file ID\n",
        "file_id = '18R9RwjJzhmm-6XuU_DxosLRtsrnkNt_Q'\n",
        "\n",
        "# Define the URL to download the file\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Define the path to save the downloaded file\n",
        "output_path = '/content/Books_rating.csv'\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url, output_path, quiet=False)\n",
        "\n",
        "# Load the CSV file into a Pandas DataFrame\n",
        "df_pandas = pd.read_csv(output_path)\n",
        "\n",
        "# Display the first few rows and columns of the DataFrame\n",
        "print(\"First few rows of the DataFrame:\")\n",
        "print(df_pandas.head())\n",
        "\n",
        "print(\"Columns in the DataFrame:\")\n",
        "print(df_pandas.columns)\n",
        "\n",
        "\n",
        "# Ensure the \"review/score\" column is of float type\n",
        "df_pandas[\"review/score\"] = df_pandas[\"review/score\"].astype(float)\n",
        "\n",
        "# Encode the 'User_id' and 'Id' columns to create 'indexed_user_id' and 'indexed_book_id'\n",
        "user_encoder = LabelEncoder()\n",
        "book_encoder = LabelEncoder()\n",
        "\n",
        "df_pandas['indexed_user_id'] = user_encoder.fit_transform(df_pandas['User_id'])\n",
        "df_pandas['indexed_book_id'] = book_encoder.fit_transform(df_pandas['Id'])\n",
        "\n",
        "# Define feature columns and target column\n",
        "feature_columns = ['indexed_user_id', 'indexed_book_id']\n",
        "target_column = 'review/score'\n",
        "\n",
        "# Convert to TensorFlow dataset\n",
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "    dataframe = dataframe.copy()\n",
        "    labels = dataframe.pop(target_column)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe[feature_columns]), labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    return ds\n",
        "\n",
        "# Split the data\n",
        "train, test = train_test_split(df_pandas, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create TensorFlow datasets\n",
        "batch_size = 32\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n",
        "\n",
        "# Define the neural network model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define input layers\n",
        "user_input = layers.Input(shape=(1,), name='user')\n",
        "book_input = layers.Input(shape=(1,), name='book')\n",
        "\n",
        "# Embedding layers\n",
        "user_embedding = layers.Embedding(input_dim=df_pandas['indexed_user_id'].max() + 1, output_dim=50)(user_input)\n",
        "book_embedding = layers.Embedding(input_dim=df_pandas['indexed_book_id'].max() + 1, output_dim=50)(book_input)\n",
        "\n",
        "# Concatenate embeddings\n",
        "concat = layers.concatenate([user_embedding, book_embedding], axis=-1)\n",
        "concat = layers.Flatten()(concat)\n",
        "\n",
        "# Fully connected layers\n",
        "dense = layers.Dense(128, activation='relu')(concat)\n",
        "dense = layers.Dense(64, activation='relu')(dense)\n",
        "output = layers.Dense(1)(dense)\n",
        "\n",
        "# Define the model\n",
        "model = tf.keras.Model(inputs=[user_input, book_input], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# Prepare the training and validation data\n",
        "train_data = {'user': train['indexed_user_id'], 'book': train['indexed_book_id']}\n",
        "train_labels = train['review/score']\n",
        "val_data = {'user': test['indexed_user_id'], 'book': test['indexed_book_id']}\n",
        "val_labels = test['review/score']\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    train_labels,\n",
        "    epochs=3,\n",
        "    validation_data=(val_data, val_labels),\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, rmse = model.evaluate(val_data, val_labels)\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "all_user_book_pairs = pd.DataFrame([(user, book) for user in df_pandas['indexed_user_id'].unique() for book in df_pandas['indexed_book_id'].unique()], columns=['indexed_user_id', 'indexed_book_id'])\n",
        "all_user_book_pairs['predicted_score'] = model.predict({'user': all_user_book_pairs['indexed_user_id'], 'book': all_user_book_pairs['indexed_book_id']})\n",
        "\n",
        "# Get top N recommendations for each user\n",
        "N = 5\n",
        "top_n_recommendations = all_user_book_pairs.groupby('indexed_user_id').apply(lambda x: x.nlargest(N, 'predicted_score')).reset_index(drop=True)\n",
        "\n",
        "# Create lookup dictionaries for book_id to title and user_id_index to user_id\n",
        "book_id_to_title = dict(df_pandas[['indexed_book_id', 'Title']].values)\n",
        "user_index_to_id = dict(df_pandas[['indexed_user_id', 'User_id']].values)\n",
        "\n",
        "# Print recommendations for each user\n",
        "for user_id in top_n_recommendations['indexed_user_id'].unique():\n",
        "    user_recs = top_n_recommendations[top_n_recommendations['indexed_user_id'] == user_id]\n",
        "    print(f\"User: {user_index_to_id[user_id]}\")\n",
        "    print(\"Top N Recommended Books:\")\n",
        "    for i, row in user_recs.iterrows():\n",
        "        book_id = row['indexed_book_id']\n",
        "        title = book_id_to_title[book_id]\n",
        "        print(f\"{i+1}. {title} (Predicted Score: {row['predicted_score']})\")\n",
        "    print(\"--------\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uiDS7XKZB6mH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Generate recommendations\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}